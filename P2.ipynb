{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich Lane Frames with Lane Perception\n",
    "---\n",
    "\n",
    "Use **Advanced Computer Vision techniques** to determine the radius of the lane, calculate vehicle position with respect to center of the lane and draw lane boundary onto original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import sys\n",
    "sys.path.append(\"/home/car/anaconda/envs/cv/lib/python3.7/site-packages\")\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.cv.CameraCalibration import CameraCalibration\n",
    "from lib.cv.GradientThresholds import GradientThresholds\n",
    "from lib.cv.ColorThresholds import ColorThresholds\n",
    "from lib.cv.CameraPerspective import CameraPerspective\n",
    "from lib.cv.LaneLineDetection import LaneLineDetection\n",
    "from lib.cv.LaneLineCurvature import LaneLineCurvature\n",
    "from lib.cv.LaneVehiclePosition import LaneVehiclePosition\n",
    "from lib.cv.LaneBoundaries import LaneBoundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "import skvideo.datasets\n",
    "from IPython.display import HTML\n",
    "from fractions import Fraction\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. **Camera Calibration**\n",
    "\n",
    "Compute camera calibration matrix and distortion coefficients given a set of chessboard images using **CameraCalibration** class's `cmpt_mtx_and_dist_coeffs()` method.\n",
    "\n",
    "2\\. **Distortion Correction**\n",
    "\n",
    "Apply distortion correction to raw images using **CameraCalibration** class's `correct_distortion()` method. First test undistortion on a raw chessboard image and visualize it. Then test undistortion on a raw curved lane line image and visualize it.\n",
    "\n",
    "3\\. **Color & Gradient Threshold**\n",
    "\n",
    "Use color transforms, gradients, etc., to create a threshold binary image.\n",
    "\n",
    "4\\. **Perspective Transform**\n",
    "\n",
    "Warp the image into a Bird's Eye View (also known as top down view).\n",
    "\n",
    "5\\. **Detect Lane Lines**\n",
    "\n",
    "Locate which pixels are lane line pixels.\n",
    "\n",
    "6\\. **Determine Lane Curvature**\n",
    "\n",
    "Calculate the left and right lane line's radius of curvature.\n",
    "\n",
    "7\\. **Determine Lane Center of Vehicle Position**\n",
    "\n",
    "Calculate vehicle's position with respect to the lane center.\n",
    "\n",
    "8\\. **Display Lane Boundaries**\n",
    "\n",
    "Visualized the warped lane boundaries back onto the undistorted image (original image right after distortion correction).\n",
    "\n",
    "9\\. **Display Lane Boundaries with Lane Curvature and Vehicle Position**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Pipeline (Image Processing)\n",
    "---\n",
    "\n",
    "Output images from each stage of the pipeline will be saved in folder `output_images`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Each Filepath and Filename into List For each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_files[0] = [filepath, filename]\n",
    "test_files = []\n",
    "for filename in os.listdir(\"test_images/\"):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        filepath = os.path.join(\"test_images/\", filename)\n",
    "        test_files.append([filepath, filename])\n",
    "\n",
    "print(*test_files, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Camera Calibration\n",
    "\n",
    "Compute Camera Calibration Matrix and Distortion Coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_corners = 9\n",
    "ny_corners = 6\n",
    "# Choose a directory with camera images of 9x6 chessboards 720x1280p(h x w)\n",
    "cam_cal_chess_dfp = \"camera_cal/calibration*.jpg\"\n",
    "smart_car_cam = CameraCalibration(nx_corners, ny_corners, cam_cal_chess_dfp)\n",
    "\n",
    "# Compute camera calibration matrix and distortion coefficients via a set of chessboard images\n",
    "src_img_fp = \"camera_cal/calibration3.jpg\"\n",
    "cal_mtx, dist_coeff = smart_car_cam.cmpt_mtx_and_dist_coeffs(src_img_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distortion Correction\n",
    "\n",
    "Apply Distortion Correction to Raw Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Visualize Corrected Distortion on a Raw Chessboard Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Distortion Correction to a raw chessboard image\n",
    "smart_car_cam.set_dist_img(src_img_fp)\n",
    "chess_dist_img, chess_undist_img = smart_car_cam.correct_distortion(cal_mtx, dist_coeff)\n",
    "\n",
    "# Save Undistorted Chessboard Image\n",
    "dst_img_fp = \"data/output/image/pipeline/distortion_correction/\"\n",
    "filename = \"calibration3.jpg\"\n",
    "smart_car_cam.save_img(dst_img_fp, filename, chess_undist_img, cal_mtx, dist_coeff)\n",
    "\n",
    "# Visualize Chessboard Image from Distorted and Undistorted \n",
    "smart_car_cam.visualize(filename, chess_dist_img, filename, chess_undist_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Visualize Corrected Distortion of each Lane Line Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dist_img_list = []\n",
    "lane_undist_img_list = []\n",
    "\n",
    "for file in test_files:\n",
    "    # First element of each row is filepath\n",
    "    # Apply Distortion Correction to each distort lane line image\n",
    "    src_img_fp = file[0]\n",
    "    smart_car_cam.set_dist_img(src_img_fp)\n",
    "    lane_dist_img, lane_undist_img = smart_car_cam.correct_distortion(cal_mtx, dist_coeff)\n",
    "    \n",
    "    print(type(lane_undist_img), lane_undist_img.shape)\n",
    "    \n",
    "    lane_dist_img_list.append(lane_dist_img)\n",
    "    lane_undist_img_list.append(lane_undist_img)\n",
    "    \n",
    "    # Visualize Chessboard Image from Distorted and Undistorted \n",
    "    smart_car_cam.visualize(file[1], lane_dist_img, file[1], lane_undist_img)\n",
    "    \n",
    "    # Save Each Undistorted lane line image using filepath\n",
    "    dst_img_fp = \"data/output/image/pipeline/distortion_correction/\"\n",
    "    smart_car_cam.save_img(dst_img_fp, file[1], lane_undist_img, cal_mtx, dist_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Color & Gradient Threshold\n",
    "\n",
    "Create a Thresholded Binary Image using Color & Gradient Thresold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_car_cam = GradientThresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Use Gradient Thresholds to Create Binary Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply X-Sobel Orientation Derivative Thresholding\n",
    "\n",
    "Identifies pixels where the gradient of an image falls within a specified threshold range. \n",
    "Can also take the Y-Sobel, but found X-Sobel to be better at picking up lane lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applies Gradient Thresholding to Undistorted Car Camera Images\n",
    "orient = 'x'\n",
    "thresh_min = 20 # 20\n",
    "thresh_max = 100 # 100\n",
    "\n",
    "# List for saving sobel-x\n",
    "sx_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    sx_binary_img = gradient_car_cam.apply_sobel_thresh(lane_undist_img, orient, thresh_min, thresh_max)\n",
    "\n",
    "    # Append sobel-x binary image to list\n",
    "    sx_binary_img_list.append(sx_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and gradient thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'X-Sobel: ' + file[1]\n",
    "    gradient_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, sx_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from gradient thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_threshold/x_sobel/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], sx_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Both X and Y Gradient Magnitude Thresholding\n",
    "\n",
    "This thresholding can be done over larger regions to smooth over noisy intensity \n",
    "fluctuations on small scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applies Gradient Magnitude Thresholding to Undistorted Lane Img\n",
    "sobel_kernel = 3\n",
    "mag_thresh = (20, 100)\n",
    "\n",
    "# List for saving gradient magnitude of undistorted lane images\n",
    "grad_mag_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    grad_mag_binary_img = gradient_car_cam.apply_grad_mag_thresh(lane_undist_img, sobel_kernel, mag_thresh)\n",
    "    \n",
    "    # Append gradient magnitude binary image to list\n",
    "    grad_mag_binary_img_list.append(grad_mag_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and gradient magnitude thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Grad Magn: ' + file[1]\n",
    "    gradient_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, grad_mag_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from gradient thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_threshold/gradient_magnitude/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], grad_mag_binary_img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Gradient Direction Thresholding\n",
    "\n",
    "This thresholding is useful for picking up edges of a particular orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_kernel = 15\n",
    "dir_thresh = (0.7, 1.3) # 0.7, 1.3\n",
    "\n",
    "# List for saving gradient direction of undistorted lane images\n",
    "grad_dir_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    grad_dir_binary_img = gradient_car_cam.apply_grad_dir_thresh(lane_undist_img, sobel_kernel, dir_thresh)\n",
    "    \n",
    "    # Append gradient direction binary image to list\n",
    "    grad_dir_binary_img_list.append(grad_dir_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and gradient direction thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Grad. Dir: ' + file[1]\n",
    "    gradient_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, grad_dir_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from gradient thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_threshold/gradient_direction/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], grad_dir_binary_img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Combined Gradient Thresholds\n",
    "\n",
    "Use various aspects of gradient measurements (x, y, magnitude, direction) to isolate lane-line pixels. First, I did a selection for pixels where x and y gradients meet the threshold criteria or the gradient magnitude and direction are both within their threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving combined gradient of undistorted lane images\n",
    "comb_grad_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, sx_binary_img, grad_mag_binary_img, grad_dir_binary_img, file in zip(lane_undist_img_list, sx_binary_img_list, grad_mag_binary_img_list, grad_dir_binary_img_list, test_files):\n",
    "    # apply_combined_thresh has combination codes:\n",
    "    # 0: X-Sobel, Gradient Magnitude\n",
    "    # 1: X-Sobel, Gradient Direction\n",
    "    # 2: X-Sobel, Gradient Magnitude, Gradient Direction\n",
    "    # 3: X-Sobel, Y-Sobel, Gradient Magnitude, Gradient Direction    \n",
    "    comb_grad_binary_img = gradient_car_cam.apply_combined_thresh(2, \n",
    "                                        grad_x = sx_binary_img,\n",
    "                                        grad_mag = grad_mag_binary_img,\n",
    "                                        grad_dir = grad_dir_binary_img)\n",
    "\n",
    "    # Append combined gradient binary image to list\n",
    "    comb_grad_binary_img_list.append(comb_grad_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined gradient thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Comb. Grad: ' + file[1]\n",
    "    gradient_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, comb_grad_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from gradient thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_threshold/combined_gradient/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], comb_grad_binary_img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Use Color Thresholds to Create Binary Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_car_cam = ColorThresholds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Grayscale Thresholding\n",
    "\n",
    "Convert to grayscale and apply a threshold to identify lane lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = (180, 255)\n",
    "\n",
    "# List for saving gray binary resulting from undistorted lane images\n",
    "gray_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    gray_binary_img = color_car_cam.apply_gray_thresh(lane_undist_img, thresh)\n",
    "    \n",
    "    # Append gradient direction binary image to list\n",
    "    gray_binary_img_list.append(gray_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and grayscale thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Grayscale: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, gray_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/grayscale/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], gray_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Invidual RGB Thresholding\n",
    "\n",
    "Using Red, Green and Blue to identify **white lane line pixels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply R Thresholding\n",
    "\n",
    "R channel does a reasonable job of highlighting lines, so you can apply threshold to find lane-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_thresh = (130, 255) # (130, 255) best so far\n",
    "\n",
    "# List for saving red binary image\n",
    "red_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    red_binary_img = color_car_cam.apply_r_thresh(lane_undist_img, r_thresh)\n",
    "    \n",
    "    # Append gradient direction binary image to list\n",
    "    red_binary_img_list.append(red_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and grayscale thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'R Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, red_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/rgb/red/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], red_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply G Thresholding\n",
    "\n",
    "Use G channel to highlight lines and apply threshold to find lane-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g_thresh = (130, 255) # (130, 255) best so far\n",
    "\n",
    "# List for saving green binary image\n",
    "green_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    green_binary_img = color_car_cam.apply_g_thresh(lane_undist_img, g_thresh)\n",
    "    \n",
    "    # Append gradient direction binary image to list\n",
    "    green_binary_img_list.append(green_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and grayscale thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'G Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, green_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/rgb/green/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], green_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply B Thresholding\n",
    "\n",
    "Use B channel to highlight lines and apply threshold to find lane-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_thresh = (195, 255) # (185, 255) best so far\n",
    "\n",
    "# List for saving green binary image\n",
    "blue_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    blue_binary_img = color_car_cam.apply_b_thresh(lane_undist_img, b_thresh)\n",
    "    \n",
    "    # Append blue binary image to list\n",
    "    blue_binary_img_list.append(blue_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and blue thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'B Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, blue_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/rgb/blue/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], blue_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Combined RGB Thresholding\n",
    "\n",
    "Combining RGB (Red, Green, Blue) to identify white lane pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving combined rgb binary images\n",
    "comb_rgb_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, red_binary_img, green_binary_img, blue_binary_img, file in zip(lane_undist_img_list, red_binary_img_list, green_binary_img_list, blue_binary_img_list, test_files):\n",
    "    # combination codes:\n",
    "    # 0: R Binary, G Binary\n",
    "    # 1: R Binary, B binary\n",
    "    # 2: G Binary, B Binary\n",
    "    # 3: R Binary, G Binary, B Binary   \n",
    "    comb_rgb_binary_img = color_car_cam.apply_rgb_thresh(3, \n",
    "                                        rgb_r = red_binary_img,\n",
    "                                        rgb_g = green_binary_img,\n",
    "                                        rgb_b = blue_binary_img)\n",
    "    \n",
    "    # Append combined rgb binary image to list\n",
    "    comb_rgb_binary_img_list.append(comb_rgb_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined gradient thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Comb. RGB: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, comb_rgb_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/rgb/comb_rgb/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], comb_rgb_binary_img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Individual HLS Thresholding\n",
    "\n",
    "Combining HLS (Hue, Lightness, Saturation) to identify **yellow lane line pixels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply H Thresholding for Detecting Yellow\n",
    "\n",
    "The lane lines appear dark in the H channel, so we try a low threshold and obtain the following result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_thresh = (20, 90) # (20, 90) best so far, detects warm red to yellow green lane line pixels\n",
    "\n",
    "# List for saving hue binary image\n",
    "hue_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    hue_binary_img = color_car_cam.apply_h_thresh(lane_undist_img, h_thresh)\n",
    "    \n",
    "    # Append saturation binary image to list\n",
    "    hue_binary_img_list.append(hue_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and saturation thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'H Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, hue_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/hls/hue/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], hue_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply L Thresholding for Detecting Yellow\n",
    "\n",
    "L channel picks up the lines well, so let's try applying a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Threshold to pick up yellow colors\n",
    "l_thresh = (120, 200) # (215, 255) good at picking up white lane line pixels\n",
    "\n",
    "# List for saving lightness binary image\n",
    "light_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    light_binary_img = color_car_cam.apply_l_thresh(lane_undist_img, l_thresh)\n",
    "    \n",
    "    # Append saturation binary image to list\n",
    "    light_binary_img_list.append(light_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and saturation thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'L Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, light_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/hls/lightness/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], light_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply S Thresholding for Detecting Yellow\n",
    "\n",
    "S channel picks up the lines well, so let's try applying a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply Threshold to pick up shades of yellow colors\n",
    "s_thresh = (100, 255)\n",
    "\n",
    "# List for saving saturation binary image\n",
    "sat_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    sat_binary_img = color_car_cam.apply_s_thresh(lane_undist_img, s_thresh)\n",
    "    \n",
    "    # Append saturation binary image to list\n",
    "    sat_binary_img_list.append(sat_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and saturation thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'S Binary: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, sat_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/hls/saturation/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], sat_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine HLS - Hue, Lightness, Saturation\n",
    "\n",
    "Right now, I have HLS set to combine Lightness and Saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined hls binary images\n",
    "comb_all_hls_binary_img_list = []\n",
    "\n",
    "for lane_undist_img, hue_binary_img, light_binary_img, sat_binary_img, file in zip(lane_undist_img_list, hue_binary_img_list, light_binary_img_list, sat_binary_img_list, test_files):\n",
    "    # combination codes:\n",
    "    # 0: H Binary, L Binary\n",
    "    # 1: H Binary, S binary\n",
    "    # 2: L Binary, S Binary\n",
    "    # 3: H Binary, L Binary, S Binary\n",
    "    comb_all_hls_binary_img = color_car_cam.apply_hls_thresh(3, \n",
    "                                        hls_h = hue_binary_img,\n",
    "                                        hls_l = light_binary_img,\n",
    "                                        hls_s = sat_binary_img)  \n",
    "    # Append combined hls binary image to list\n",
    "    comb_all_hls_binary_img_list.append(comb_all_hls_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined gradient thresholded image\n",
    "    src_title = 'Undistorted: ' + file[1]\n",
    "    thresh_img_title = 'Comb. HLS: ' + file[1]\n",
    "    color_car_cam.visualize(src_title, lane_undist_img, thresh_img_title, comb_all_hls_binary_img)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/color_threshold/hls/comb_all_hls/\"\n",
    "    color_car_cam.save_img(dst_img_fp, file[1], comb_all_hls_binary_img)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Thresholding Summary\n",
    "\n",
    "From the color thresholding above for RGB and HSL, you can see \n",
    "H channel is probably your best bet. It's near the same result\n",
    "as S channel and a bit better than the R channel or simple grayscaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Color & Gradient Thresholding\n",
    "\n",
    "We will see which parts of the lane lines were detected by the gradient threshold and which parts were detected by color threshold by stacking the channels and seeing the individual components.\n",
    "\n",
    "We will create a binary combination of these two images to map out where either the color or gradient thresholds were met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Gradient Sobel X and HLS Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving combined Sobel-X and Saturation binary images\n",
    "comb_sx_sat_binary_img_list = []\n",
    "# List for saving stacked Sobel-X and Saturation color images\n",
    "stack_sx_sat_color_img_list = []\n",
    "\n",
    "for lane_undist_img, sx_binary_img, sat_binary_img, file in zip(lane_undist_img_list, sx_binary_img_list, sat_binary_img_list, test_files):\n",
    "    # Stack Scaled Sobel X and S Color Channel:\n",
    "    # to view their individual contributions in green and blue respectively\n",
    "    # Green color is sobel-x gradient threshold component\n",
    "    # Blue color is saturation threshold component \n",
    "    sx_sat_color_binary = np.dstack(( np.zeros_like(sx_binary_img), sx_binary_img, sat_binary_img )) * 255\n",
    "\n",
    "    # Append stacked Sobel-X and Saturation color image to list\n",
    "    stack_sx_sat_color_img_list.append(sx_sat_color_binary)\n",
    "    \n",
    "    # Combine the two binary thresholds\n",
    "    comb_sx_sat_binary_img = np.zeros_like(sx_binary_img)\n",
    "    comb_sx_sat_binary_img[ (sx_binary_img == 1) | (sat_binary_img == 1) ] = 1\n",
    "\n",
    "    # Append combined Sobel-X and Saturation binary image to list\n",
    "    comb_sx_sat_binary_img_list.append(comb_sx_sat_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined gradient thresholded image\n",
    "    src_title = 'Stack. S-X_SAT: ' + file[1]\n",
    "    thresh_img_title = 'Comb. S-X_SAT: ' + file[1]\n",
    "    gradient_car_cam.visualize(src_title, sx_sat_color_binary, thresh_img_title, comb_sx_sat_binary_img)\n",
    "    \n",
    "    # Save each stacked color image\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/stacked_sobelx_sat/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], sx_sat_color_binary)\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/combined_sobelx_sat/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], comb_sx_sat_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine HLS and RGB\n",
    "\n",
    "HLS identified yellow lane line pixels and RGB identified white lane line pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving combined hls, rgb binary images\n",
    "comb_hls_rgb_binary_img_list = []\n",
    "# List for saving stacked Sobel-X and Saturation color images\n",
    "stack_hls_rgb_color_img_list = []\n",
    "\n",
    "for lane_undist_img, comb_all_hls_binary_img, comb_rgb_binary_img, file in zip(lane_undist_img_list, comb_all_hls_binary_img_list, comb_rgb_binary_img_list, test_files):\n",
    "    # Stack HLS and RGB Color Channel:\n",
    "    # to view their individual contributions in green and blue respectively\n",
    "    # Green color is HLS threshold component\n",
    "    # Blue color is RGB threshold component\n",
    "    hls_rgb_color_binary = np.dstack(( np.zeros_like(comb_all_hls_binary_img), comb_all_hls_binary_img, comb_rgb_binary_img )) * 255\n",
    "\n",
    "    # Append stacked HLS, RGB color image to list\n",
    "    stack_hls_rgb_color_img_list.append(hls_rgb_color_binary)\n",
    "    \n",
    "    # Combine the three (HLS, RGB) binary thresholds\n",
    "    comb_hls_rgb_binary_img = np.zeros_like(comb_all_hls_binary_img)\n",
    "    comb_hls_rgb_binary_img[ (comb_all_hls_binary_img == 1) |\n",
    "                             (comb_rgb_binary_img == 1) ] = 1\n",
    "\n",
    "    # Append combined HLS, RGB binary image to list\n",
    "    comb_hls_rgb_binary_img_list.append(comb_hls_rgb_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined HLS + RGB binary image\n",
    "    stack_title = 'HLS_RGB: ' + file[1]\n",
    "    thresh_title = 'HLS_RGB: ' + file[1]\n",
    "    gradient_car_cam.visualize(stack_title, hls_rgb_color_binary, thresh_title, comb_hls_rgb_binary_img)\n",
    "    \n",
    "    # Save each stacked HLS + RGB binary image\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/stacked_hls_rgb/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], hls_rgb_color_binary)\n",
    "    \n",
    "    # Save each binary image resulting from HLS + RGB binary image\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/combined_hls_rgb/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], comb_hls_rgb_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Gradient with RGB and HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving combined gradient, rgb, hls binary images\n",
    "comb_grad_hls_rgb_binary_img_list = []\n",
    "# List for saving stacked gradient, rgb, hls color images\n",
    "stack_grad_hls_rgb_color_img_list = []\n",
    "\n",
    "for comb_grad_binary_img, comb_hls_rgb_binary_img, file in zip(comb_grad_binary_img_list, comb_hls_rgb_binary_img_list, test_files):\n",
    "    # Stack Combined Gradient, RGB, HLS Color Channel:\n",
    "    # to view their individual contributions in green and blue respectively\n",
    "    # Green color is combined Gradient threshold component\n",
    "    # Blue color is RGB, HLS threshold component\n",
    "    stack_grad_hls_rgb_color_img = np.dstack(( np.zeros_like(comb_grad_binary_img), comb_grad_binary_img, comb_hls_rgb_binary_img )) * 255\n",
    "\n",
    "    # Append stacked combined Gradient, HLS, RGB color image to list\n",
    "    stack_grad_hls_rgb_color_img_list.append(stack_grad_hls_rgb_color_img)\n",
    "    \n",
    "    # Combine the three (Gradient, HLS, RGB) binary thresholds\n",
    "    comb_grad_hls_rgb_binary_img = np.zeros_like(comb_grad_binary_img)\n",
    "    comb_grad_hls_rgb_binary_img[ (comb_grad_binary_img == 1) |\n",
    "                                  (comb_hls_rgb_binary_img == 1) ] = 1\n",
    "\n",
    "    # Append combined Gradient, HLS, RGB binary image to list\n",
    "    comb_grad_hls_rgb_binary_img_list.append(comb_grad_hls_rgb_binary_img)\n",
    "    \n",
    "    # Visualizes a figure with undistorted image and combined HLS + RGB binary image\n",
    "    stack_title = 'Grad_HLS_RGB: ' + file[1]\n",
    "    thresh_title = 'Grad_HLS_RGB: ' + file[1]\n",
    "    gradient_car_cam.visualize(stack_title, stack_grad_hls_rgb_color_img, thresh_title, comb_grad_hls_rgb_binary_img)\n",
    "    \n",
    "    # Save each stacked Gradient, HLS + RGB binary image\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/stacked_grad_hls_rgb/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], stack_grad_hls_rgb_color_img)\n",
    "    \n",
    "    # Save each binary image resulting from HLS + RGB binary image\n",
    "    dst_img_fp = \"data/output/image/pipeline/gradient_color_threshold/combined_grad_hls_rgb/\"\n",
    "    gradient_car_cam.save_img(dst_img_fp, file[1], comb_grad_hls_rgb_binary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perspective Transform\n",
    "\n",
    "Identify four source points in an image where the lane lines are straight. After perspective transform, make the lines look straight and vertical from a bird's eye view perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_cam_view = CameraPerspective()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Camera View to Bird's Eye View\n",
    "\n",
    "### 4A. Show Source Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List for saving source points used in Bird's Eye View\n",
    "mark_src_pts_img_list = []\n",
    "\n",
    "for lane_undist_img, file in zip(lane_undist_img_list, test_files):\n",
    "    imshape = lane_undist_img.shape\n",
    "        \n",
    "#     plt.imshow(lane_undist_img)\n",
    "#     plt.plot(imshape[1]*0.145, imshape[0], '.', markersize = 12) # bottom left\n",
    "#     plt.plot(imshape[1]*0.462, imshape[0]*0.62, '.', markersize = 12) # top left\n",
    "#     plt.plot(imshape[1]*0.535, imshape[0]*0.62, '.', markersize = 12) # bottom right\n",
    "#     plt.plot(imshape[1]*0.883, imshape[0], '.', markersize = 12) # bottom right\n",
    "\n",
    "    # Notice there are two straight lane line images, those are my\n",
    "    # two base images for choosing source points in warping image\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(lane_undist_img, cmap = 'gray')\n",
    "    ax1.set_title(\"Undistorted: \" + file[1], fontsize=30)\n",
    "    ax2.imshow(lane_undist_img, cmap = 'gray')\n",
    "    ax2.set_title(\"Source Pts: \" + file[1], fontsize=30)\n",
    "    ax2.plot(imshape[1]*0.145, imshape[0], '.', markersize = 25) # bottom left\n",
    "    ax2.plot(imshape[1]*0.462, imshape[0]*0.62, '.', markersize = 25) # top left\n",
    "    ax2.plot(imshape[1]*0.535, imshape[0]*0.62, '.', markersize = 25) # bottom right\n",
    "    ax2.plot(imshape[1]*0.883, imshape[0], '.', markersize = 25) # bottom right    \n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/perspective_transform/source_points/\"\n",
    "    car_cam_view.save_fig(dst_img_fp, file[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Warp Lane Images to Bird's Eye View\n",
    "\n",
    "Uses Combined Sobel-X, Hue, Saturation Binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List for saving lane line images warped to Bird's Eye View\n",
    "warped_lane_bev_img_list = []\n",
    "\n",
    "for comb_grad_hls_rgb_binary_img, file in zip(comb_grad_hls_rgb_binary_img_list, test_files):\n",
    "    warped_lane_bev_img = car_cam_view.birds_eye_view(comb_grad_hls_rgb_binary_img)\n",
    "\n",
    "    # Add Bird's Eye View lane line images\n",
    "    warped_lane_bev_img_list.append(warped_lane_bev_img)\n",
    "    \n",
    "    # Visualizes a figure with combined gradient and color thresholded image\n",
    "    # with warped birds eye view image\n",
    "    src_title = 'Grad_HLS_RGB: ' + file[1]\n",
    "    warp_img_title = 'Warped. BEV: ' + file[1]\n",
    "    car_cam_view.visualize(src_title, comb_grad_hls_rgb_binary_img, warp_img_title, warped_lane_bev_img)    \n",
    "    \n",
    "    # Save each binary image resulting from warped birds eye view\n",
    "    dst_img_fp = \"data/output/image/pipeline/perspective_transform/birds_eye_view/\"\n",
    "    car_cam_view.save_img(dst_img_fp, file[1], warped_lane_bev_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4C. Show Destination Points for Warped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for saving destination points used in Bird's Eye View\n",
    "mark_dst_pts_img_list = []\n",
    "\n",
    "for comb_grad_hls_rgb_binary_img, warped_lane_bev_img, file in zip(comb_grad_hls_rgb_binary_img_list, warped_lane_bev_img_list, test_files):\n",
    "    imshape = comb_grad_hls_rgb_binary_img.shape\n",
    "\n",
    "    # Notice there are two straight lane line images, those are my\n",
    "    # two base images for choosing source points in warping image\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.imshow(comb_grad_hls_rgb_binary_img, cmap = 'gray')\n",
    "    ax1.set_title(\"Source Pts: \" + file[1], fontsize=30)\n",
    "    ax1.plot(imshape[1]*0.145, imshape[0], '.', markersize = 25) # bottom left\n",
    "    ax1.plot(imshape[1]*0.462, imshape[0]*0.62, '.', markersize = 25) # top left\n",
    "    ax1.plot(imshape[1]*0.535, imshape[0]*0.62, '.', markersize = 25) # bottom right\n",
    "    ax1.plot(imshape[1]*0.883, imshape[0], '.', markersize = 25) # bottom right      \n",
    "    \n",
    "    ax2.imshow(warped_lane_bev_img, cmap = 'gray')\n",
    "    ax2.set_title(\"Destn Pts: \" + file[1], fontsize=30)\n",
    "    ax2.plot(imshape[1]*0.24, imshape[0], '.', markersize = 25) # bottom left\n",
    "    ax2.plot(imshape[1]*0.24, imshape[0]*0, '.', markersize = 25) # top left\n",
    "    ax2.plot(imshape[1]*0.75, imshape[0]*0, '.', markersize = 25) # bottom right\n",
    "    ax2.plot(imshape[1]*0.75, imshape[0], '.', markersize = 25) # bottom right\n",
    "    \n",
    "    # Save each binary image resulting from color thresholding\n",
    "    dst_img_fp = \"data/output/image/pipeline/perspective_transform/destination_points/\"\n",
    "    car_cam_view.save_fig(dst_img_fp, file[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detect Lane Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5A. Histogram Peaks\n",
    "\n",
    "Now that we have the thresholded warped image, we can map out the lane lines! There are multiple ways to go about doing this, we'll use a **histogram** to see where the binary activations (peaks) occur across the image, so we can decide explicitly which pixels are part of the lines and which belong to the left line and which belong to the right line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of the finding lane lines class\n",
    "find_lane_lines = LaneLineDetection()\n",
    "\n",
    "# Histogram list for saving histograms for each lane line image\n",
    "histogram_list = []\n",
    "\n",
    "# Select either to visualize only histogram or histogram and lane lines\n",
    "only_histogram = False\n",
    "lane_bev_and_histogram = True\n",
    "\n",
    "for warped_lane_bev_img, file in zip(warped_lane_bev_img_list, test_files):\n",
    "    # Create histogram of image binary activations\n",
    "    histogram = find_lane_lines.histogram_peaks(warped_lane_bev_img)\n",
    "\n",
    "    # Append histogram to list\n",
    "    histogram_list.append(histogram)\n",
    "    \n",
    "    # Titles to use in the below visualization\n",
    "    warp_img_title = 'Warped. BEV: ' + file[1]\n",
    "    hist_title = 'Histogram: ' + file[1]\n",
    "    if only_histogram == True and lane_bev_and_histogram == True:\n",
    "        # Visualize only the resulting histogram\n",
    "        find_lane_lines.visualize_hist(hist_title, histogram)\n",
    "        # Save only histogram plotted from birds eye view lane line image\n",
    "        dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/histogram/\"\n",
    "        find_lane_lines.save_fig(dst_img_fp, file[1])\n",
    "        # show figure of only histogram separately\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualize the lane line birds eye view image and histogram\n",
    "        find_lane_lines.visualize_lanes_and_hist(\n",
    "            warp_img_title, \n",
    "            warped_lane_bev_img,\n",
    "            hist_title,\n",
    "            histogram)\n",
    "        # Save only histogram plotted from birds eye view lane line image\n",
    "        dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/lanes_and_hist/\"\n",
    "        find_lane_lines.save_fig(dst_img_fp, file[1])\n",
    "        # show figure of lane birds eye view next to histogram separately\n",
    "        plt.show()    \n",
    "    elif only_histogram == True:\n",
    "        # Visualize only the resulting histogram\n",
    "        find_lane_lines.visualize_hist(hist_title, histogram)\n",
    "        # Save only histogram plotted from birds eye view lane line image\n",
    "        dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/histogram/\"\n",
    "        find_lane_lines.save_fig(dst_img_fp, file[1])\n",
    "    elif lane_bev_and_histogram == True:\n",
    "        # Visualize the lane line birds eye view image and histogram\n",
    "        find_lane_lines.visualize_lanes_and_hist(\n",
    "            warp_img_title, \n",
    "            warped_lane_bev_img,\n",
    "            hist_title,\n",
    "            histogram)\n",
    "        # Save only histogram plotted from birds eye view lane line image\n",
    "        dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/lanes_and_hist/\"\n",
    "        find_lane_lines.save_fig(dst_img_fp, file[1])\n",
    "    else:\n",
    "        print(\"None of the options were seleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5B. Sliding Windows\n",
    "\n",
    "We use sliding windows moving upward in the image (further along the road) to determine where the lane lines go, essentially we are finding the lane line pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store lane line imgs with sliding windows and fitted polynomial\n",
    "out_img_list = []\n",
    "# Store left fit polynomial in pixels\n",
    "left_fitx_list = []\n",
    "# Store right fit polynomial in pixels\n",
    "right_fitx_list = []\n",
    "\n",
    "for warped_lane_bev_img, histogram, file in zip(warped_lane_bev_img_list, histogram_list, test_files):\n",
    "    # Uses Histogram peaks and Sliding Window method to find all pixels\n",
    "    # belonging to each line (left and right line)\n",
    "    out_img = find_lane_lines.find_lane_pixels(warped_lane_bev_img, histogram)\n",
    "    \n",
    "    # Fits a polynomial to each line in pixels\n",
    "    left_fitx, right_fitx = find_lane_lines.fit_polynomial(warped_lane_bev_img)\n",
    "    \n",
    "    # Append left fit polynomial in pixels to left line\n",
    "    left_fitx_list.append(left_fitx)\n",
    "    # Append right fit polynomial in pixels to right line\n",
    "    right_fitx_list.append(right_fitx)\n",
    "\n",
    "    # Visualize the sliding windows and fit polynomial per line\n",
    "    fig_title = \"Sliding Windows: \" + file[1]\n",
    "    find_lane_lines.visualize_fit_polynomial(out_img, left_fitx, right_fitx, fig_title)\n",
    "    \n",
    "    # Append lane line imgs with sliding windows and fitted polynomial\n",
    "    out_img_list.append(out_img)\n",
    "    \n",
    "    # Save lane line sliding window images\n",
    "    dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/sliding_windows/\"\n",
    "    find_lane_lines.save_img(dst_img_fp, file[1], out_img)\n",
    "    \n",
    "    # Save lane line images with sliding windows and fit polynomials\n",
    "    dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/fit_polynomials/\"\n",
    "    find_lane_lines.save_fig(dst_img_fp, file[1])\n",
    "    \n",
    "    # Show each figure individually\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5C. Search from Prior\n",
    "\n",
    "We've built an algorithm that uses sliding windows to track the lane lines out into the distance. Yet, using the algorithm everytime to start fresh on every frame may seem inefficient since the lines don't necessarily move alot from frame to frame. So, in the next frame of video, we can just search in a margin around the previous line position. Thus, once we know where the lines are in one frame of video, we can do a highly targeted search for them in the next frame. This is equivalent to using a customized region of interest for each frame of video and should help with tracking the lanes through sharp cuves and tricky conditions. If we lose track of the lines, we can go back to sliding windows search to rediscover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store search from prior left fit polynomial in pixels\n",
    "left_fitx_sfp_list = []\n",
    "# Store search from prior right fit polynomial in pixels\n",
    "right_fitx_sfp_list = []\n",
    "\n",
    "for warped_lane_bev_img, left_fitx, right_fitx, file in zip(warped_lane_bev_img_list, left_fitx_list, right_fitx_list, test_files):\n",
    "    # Search from prior polynomials to get left_fitx and right_fitx\n",
    "    left_fitx, right_fitx = find_lane_lines.search_around_poly(warped_lane_bev_img)\n",
    "    find_lane_lines.visualize_sap(warped_lane_bev_img, left_fitx, right_fitx)\n",
    "    \n",
    "    left_fitx_sfp_list.append(left_fitx)\n",
    "    right_fitx_sfp_list.append(right_fitx)\n",
    "    \n",
    "    # Save lane line images with search from prior fit polynomials\n",
    "    dst_img_fp = \"data/output/image/pipeline/detect_lane_lines/search_from_prior/\"\n",
    "    find_lane_lines.save_fig(dst_img_fp, file[1])   \n",
    "    \n",
    "    # Show individual figures\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Windows vs Search From Prior\n",
    "\n",
    "Notice the sliding windows compared to search from prior polynomial takes the last fit polynomial to lane line pixels and uses it to place on new frames with lane lines. When the search from prior approach doesnt match well with the actual direction of lane lines, then sliding windows approach needs to be applied again to find the lane line pixels, then replot a new polynomial to each line, so the lane lines are accurately detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use both Sliding Windows and Search from Prior Techniques\n",
    "# 1. If first time detecting lane lines, then use Sliding Windows\n",
    "\n",
    "# 2. If second or more times, then try Search from Prior\n",
    "# 3. If Search from Prior technique is too far off from actual lane\n",
    "# lines, then re-use Sliding Windows to get the best fit polynomial \n",
    "# to each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure Lane Line Curvature\n",
    "\n",
    "We have polynomial fits and can calculate the radius of curvature. We will use LaneLineDetection class's **fet_fit_polynomial_data()** to pass the needed data to LaneLineCurvature class's **measure_curvature_pixels()**. The method uses Rcurve equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order Polynomial\n",
    "\n",
    "Lane line pixels were located and their x and y pixel positions were used to fit a second order polynomial curve:\n",
    "\n",
    "f(y) = Ay^2 + By + C\n",
    "\n",
    "> Note: f(y) was fit rather than f(x) since the lane lines in the warped image are near vertical.\n",
    "\n",
    "### Radius of Curvature\n",
    "\n",
    "The radius of curvature at any point x of the function x = f(y):\n",
    "\n",
    "Rcurve = ([1 + (dx/dy)^2]^(3/2))/(|(d^(2)x)/(dy^(2))|)\n",
    "\n",
    "In the case of the second order polynomial, the 1st and 2nd derivatives are:\n",
    "\n",
    "f'(y) = (dx)/(dy) = 2Ay + B\n",
    "f''(y) = (d^(2)x)/(dy^(2)) = 2A\n",
    "\n",
    "So, our equation for radius of curvature:\n",
    "\n",
    "**Rcurve** = ((1 + (2Ay + B)^2)^(3/2))/(|2A|)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lane Line Curvature from Pixels to Real-World\n",
    "\n",
    "Converting our x and y values to real world space involves measuring how long and wide the section of lane is we're projecting in our warped image. **We can do this by measuring out the physical lane in the field of view of the camera.** Yet, for this project, we can assume if we're projecting a section of lane similar to the images above, the lane is about 30 meters long and 3.7 meters wide.\n",
    "\n",
    "> Note: If you prefer to derive a conversion from pixel space to world space in your own images, compare your images with U.S. regulations that require a minimum lane width of 12 feet or 3.7 meters and the dashed lane lines are 10 feet or 3 meters long each.\n",
    "\n",
    "### Conversion Pixel space to Real-World Meters\n",
    "\n",
    "Our camera image has 720 pixels in the y-dimension\n",
    "and 700 pixels in the x-dimension. 200 pixels on the left to 900 on the right were used, so 900-200 = 700 pixels.\n",
    "\n",
    "~~~python\n",
    "ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "\n",
    "xm_per_pixel = 3.7/700 # meters per pixel in x dimension\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_lane_curve = LaneLineCurvature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store left line curvature to list\n",
    "left_curverad_list = []\n",
    "# Store right line curvature to list\n",
    "right_curverad_list = []\n",
    "# Store lane line curvature units\n",
    "lane_curverad_units = None\n",
    "\n",
    "# Applying Sliding Windows first, then calculate lane curvature\n",
    "for warped_lane_bev_img, histogram, file in zip(warped_lane_bev_img_list, histogram_list, test_files):\n",
    "    # Uses Histogram peaks and Sliding Window method to find all pixels\n",
    "    # belonging to each line (left and right line)\n",
    "    out_img = find_lane_lines.find_lane_pixels(warped_lane_bev_img, histogram)\n",
    "    \n",
    "    # Fits a polynomial to each line in pixels\n",
    "    find_lane_lines.fit_polynomial(warped_lane_bev_img)\n",
    "    \n",
    "    # Get data from each fit polynomial\n",
    "    ploty, left_fit, right_fit = find_lane_lines.get_fit_polynomial_data()  \n",
    "    \n",
    "    # Measures Lane Line Curvature\n",
    "    left_curverad, right_curverad, curverad_units = calc_lane_curve.measure_radius_curvature(ploty, left_fit, right_fit, \"meters\")\n",
    "    \n",
    "    # Store left line curvature to list\n",
    "    left_curverad_list.append(left_curverad)\n",
    "    # Store right line curvature to list\n",
    "    right_curverad_list.append(right_curverad)\n",
    "    # Store lane line curvature units to list\n",
    "    lane_curverad_units = curverad_units\n",
    "    \n",
    "    # Displays radius of curvature\n",
    "    frame_title = file[1]\n",
    "    calc_lane_curve.display_radius_curvature(frame_title)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Determine Lane Center of Vehicle Position\n",
    "\n",
    "Calculate the vehicle's position with respect to lane center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_vehicle = LaneVehiclePosition()\n",
    "\n",
    "# Store ploty\n",
    "ploty_list = []\n",
    "# Store left fit polynomial in pixels\n",
    "left_fit_list = []\n",
    "# Store right fit polynomial in pixels\n",
    "right_fit_list = []\n",
    "\n",
    "# Store vehicle's distance from the center of the lane to list\n",
    "dist_center_list = []\n",
    "# Store vehicle's position units\n",
    "vehicle_position_units = None\n",
    "# Store if vehicle's position is left or right of the center\n",
    "side_center_list = []\n",
    "\n",
    "# Applying Sliding Windows first, then calculate lane curvature\n",
    "for warped_lane_bev_img, histogram, file in zip(warped_lane_bev_img_list, histogram_list, test_files):\n",
    "    # Uses Histogram peaks and Sliding Window method to find all pixels\n",
    "    # belonging to each line (left and right line)\n",
    "    out_img = find_lane_lines.find_lane_pixels(warped_lane_bev_img, histogram)\n",
    "    \n",
    "    # Fits a polynomial to each line in pixels\n",
    "    find_lane_lines.fit_polynomial(warped_lane_bev_img)\n",
    "    \n",
    "    # Get data from each fit polynomial\n",
    "    ploty, left_fit, right_fit = find_lane_lines.get_fit_polynomial_data()\n",
    "\n",
    "    # Append each ploty to list\n",
    "    ploty_list.append(ploty)\n",
    "    # Append each left fit polynomial to list\n",
    "    left_fit_list.append(left_fit)\n",
    "    # Append each right fit polynomial to list    \n",
    "    right_fit_list.append(right_fit)    \n",
    "    \n",
    "    # Measure Vehicle's position with respect to lane center\n",
    "    dist_center, position_units, side_center = lane_vehicle.measure_vehicle_position(warped_lane_bev_img, left_fit, right_fit, \"meters\")\n",
    "\n",
    "    # Append vehicle's distance from center of the lane\n",
    "    dist_center_list.append(dist_center)\n",
    "    \n",
    "    # Save vehicle's distance from lane center in units\n",
    "    vehicle_position_units = position_units\n",
    "    \n",
    "    # Append string of whether vehicle is left or right of lane center\n",
    "    side_center_list.append(side_center)\n",
    "    \n",
    "    # Print Vehicle's Position\n",
    "    frame_title = file[1]    \n",
    "    lane_vehicle.display_vehicle_position(frame_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Display Lane Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lane_boundary = LaneBoundaries()\n",
    "\n",
    "# Store images with lane boundary overlayed to list\n",
    "lane_boundary_img_list = []\n",
    "\n",
    "# Overlay lane boundary onto warped lane images, then display them\n",
    "for lane_undist_img, warped_lane_bev_img, histogram, ploty, left_fit, right_fit, file in zip(lane_undist_img_list, warped_lane_bev_img_list, histogram_list, ploty_list, left_fit_list, right_fit_list, test_files):\n",
    "    # Set binary image curved_lane_td in lane_boundary\n",
    "    lane_boundary.set_warped_binary_img(warped_lane_bev_img)\n",
    "\n",
    "    # Set undistorted image lane_undist_img\n",
    "    lane_boundary.set_original_undist_img(lane_undist_img)\n",
    "\n",
    "    # Set attributes related to fit the lines with a polynomial\n",
    "    lane_boundary.set_fit_lines_poly(ploty, left_fit, right_fit)\n",
    "\n",
    "    # Get inverse perspective matrix\n",
    "    Minv = car_cam_view.get_minv()\n",
    "\n",
    "    # Set inverse perspective matrix (Minv)\n",
    "    lane_boundary.set_minv(Minv)\n",
    "\n",
    "    # Overlay Detected Lane Boundaries back onto original image\n",
    "    lane_boundary.overlay_lane_boundaries()\n",
    "\n",
    "    # Visualize the overlay lane boundaries\n",
    "    lane_boundary.visualize()\n",
    "    # Get overlayed boundary image result\n",
    "    lane_boundary_img = lane_boundary.get_overlayed_image()\n",
    "    \n",
    "    # Append lane image with lane boundary overlayed\n",
    "    lane_boundary_img_list.append(lane_boundary_img)\n",
    "    \n",
    "    # Save lane images overlayed with lane boundary highlighted\n",
    "    dst_img_fp = \"data/output/image/pipeline/lane_perception/lane_boundary/\"\n",
    "    lane_boundary.save_img(dst_img_fp, file[1], lane_boundary_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Display Lane Boundaries with Lane Curvature and Vehicle Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets text properties to configure how text is displayed in image\n",
    "font_family=cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_color=(255,255,255)\n",
    "font_size=1.6\n",
    "font_thickness = 3\n",
    "line_type = cv2.LINE_AA\n",
    "lane_boundary.set_img_text_properties(font_family, font_color, font_size, font_thickness, line_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay Lane Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store lane images with lane boundary and lane curvature overlayed\n",
    "lane_bound_curve_img_list = []\n",
    "\n",
    "# Overlay lane boundary onto warped lane images, then display them\n",
    "for lane_boundary_img, left_curverad, right_curverad, file in zip(lane_boundary_img_list, left_curverad_list, right_curverad_list, test_files):\n",
    "    # Set the overlayed lane boundary image to be used by lane_boundary\n",
    "    lane_boundary.set_overlayed_lane_boundary(lane_boundary_img)\n",
    "\n",
    "    # Sets left and right lane curvature radius and units\n",
    "    lane_boundary.set_lane_curvature_radius(left_curverad, right_curverad, lane_curverad_units)\n",
    "\n",
    "    # Adds Lane Curvature Radius text to lane boundaries image\n",
    "    lane_boundary.overlay_lane_curvature()\n",
    "\n",
    "    # Visualizes the lane boundaries image with lane curvature added\n",
    "    lane_boundary.visualize()\n",
    "    \n",
    "    # Get overlayed lane boundary and curvature image result\n",
    "    lane_bound_curve_img = lane_boundary.get_overlayed_image()\n",
    "    \n",
    "    # Append lane image with lane boundary and curvature overlayed\n",
    "    lane_bound_curve_img_list.append(lane_bound_curve_img)\n",
    "    \n",
    "    # Save lane images overlayed with lane boundary highlighted\n",
    "    dst_img_fp = \"data/output/image/pipeline/lane_perception/lane_curvature/\"\n",
    "    lane_boundary.save_img(dst_img_fp, file[1], lane_bound_curve_img)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay Vehicle Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store lane images with lane boundary, lane curvature \n",
    "# and vehicle position with respect to lane center overlayed\n",
    "lane_perception_img_list = []\n",
    "\n",
    "# Overlay lane boundary onto warped lane images, then display them\n",
    "for lane_bound_curve_img, dist_center, side_center, file in zip(lane_bound_curve_img_list, dist_center_list, side_center_list, test_files):\n",
    "    # Set the overlayed lane boundary with curvature image to be used by lane_boundary\n",
    "    lane_boundary.set_overlayed_lane_boundary(lane_bound_curve_img)\n",
    "\n",
    "    # Set Vehicle Position with respect to center of lane\n",
    "    lane_boundary.set_vehicle_position(dist_center, vehicle_position_units, side_center)\n",
    "\n",
    "    # Adds Vehicle Position text to lane boundaries image\n",
    "    lane_boundary.overlay_vehicle_position()\n",
    "\n",
    "    # Visualizes the lane boundaries image with vehicle position added\n",
    "    lane_boundary.visualize()    \n",
    "    \n",
    "    # Get overlayed lane boundary, lane curvature and vehicle position\n",
    "    # image result\n",
    "    lane_perception_img = lane_boundary.get_overlayed_image()\n",
    "    \n",
    "    # Append lane image with lane boundary, lane curvature \n",
    "    # and vehicle position overlayed\n",
    "    lane_perception_img_list.append(lane_perception_img)\n",
    "    \n",
    "    # Save lane images overlayed with lane boundary highlighted\n",
    "    dst_img_fp = \"data/output/image/pipeline/lane_perception/vehicle_position/\"\n",
    "    lane_boundary.save_img(dst_img_fp, file[1], lane_perception_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Pipeline (Video Processing)\n",
    "---\n",
    "\n",
    "Modify each frame from video for **lane perception** by drawing **lane \n",
    "boundary**, **radius of lane curvature** and **vehicle position with respect to center of lane**.\n",
    "\n",
    "I must test my pipeline on video:\n",
    "\n",
    "- `project_video.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_pipeline detects radius of lane curvature, vehicle position \n",
    "# with respect to center of lane and fills in the lane boundary\n",
    "# over the original undistorted image\n",
    "def run_pipeline(frame):\n",
    "    ##\n",
    "    # 1. Camera Calibration\n",
    "    ##\n",
    "    nx_corners = 9\n",
    "    ny_corners = 6\n",
    "    \n",
    "    # Choose a directory with camera images of 9x6 chessboards 720x1280p(h x w)\n",
    "    cam_cal_chess_dfp = \"camera_cal/calibration*.jpg\"\n",
    "    calibrate_cam = CameraCalibration(nx_corners, ny_corners, cam_cal_chess_dfp)\n",
    "\n",
    "    # Compute camera calibration matrix and distortion coefficients via a set of chessboard images\n",
    "    src_img_fp = \"camera_cal/calibration3.jpg\"\n",
    "    cal_mtx, dist_coeff = calibrate_cam.cmpt_mtx_and_dist_coeffs(src_img_fp)\n",
    "    \n",
    "    ##\n",
    "    # 2. Image Distortion Correction\n",
    "    ##\n",
    "    \n",
    "    # Apply Distortion Correction to a raw lane line frame\n",
    "    dist_frame, undist_frame = calibrate_cam.correct_distortion(cal_mtx, dist_coeff, frame)\n",
    "\n",
    "    ##\n",
    "    # 3. Color & Gradient Thresholding\n",
    "    ##\n",
    "    \n",
    "    # Applies X-Sobel Gradient Threshold to Undistorted Camera Frame\n",
    "    orient = 'x'\n",
    "    thresh_min = 20\n",
    "    thresh_max = 100\n",
    "    gradient_cam = GradientThresholds()\n",
    "    sx_binary_frame = gradient_car_cam.apply_sobel_thresh(undist_frame, orient, thresh_min, thresh_max)   \n",
    "    \n",
    "    # Applies S Threshold to Undistorted Camera Frame\n",
    "    # since it more easily identifies bright color lane lines\n",
    "    color_cam = ColorThresholds()\n",
    "    s_thresh = (100, 255)\n",
    "    s_binary_frame = color_cam.apply_s_thresh(undist_frame, s_thresh)\n",
    "\n",
    "    # Combine X-Sobel and S Thresold\n",
    "    # Stack Scaled Sobel X and S Color Channel:\n",
    "    # to view their individual contributions in green and blue respectively\n",
    "    xs_color_binary = np.dstack(( np.zeros_like(sx_binary_frame), sx_binary_frame, s_binary_frame )) * 255\n",
    "\n",
    "    # Combine the two binary thresholds\n",
    "    combined_binary = np.zeros_like(sx_binary_frame)\n",
    "    combined_binary[ (s_binary_frame == 1) | (sx_binary_frame == 1) ] = 1\n",
    "\n",
    "    ##\n",
    "    # 4. Perspective Transform to Bird Eye View\n",
    "    ##\n",
    "    \n",
    "    cam_view = CameraPerspective()\n",
    "    \n",
    "    # Change Lane Line Frame to Bird's Eye View, still binary frame\n",
    "    lane_b_e_view = cam_view.birds_eye_view(combined_binary)  \n",
    "\n",
    "    ##\n",
    "    # 5. Detect Lane Lines (Enhanced)\n",
    "    ##\n",
    "    \n",
    "    # Create an instance of the finding lane lines class\n",
    "    find_lane_lines = LaneLineDetection()\n",
    "\n",
    "    # First use histogram peaks to determine where lane lines start:\n",
    "    # Create histogram of image binary activations\n",
    "    histo = find_lane_lines.histogram_peaks(lane_b_e_view)\n",
    "    \n",
    "    # Second use Sliding Window method to find all lane pixels:\n",
    "    # belonging to each line (left and right line)\n",
    "    out_img = find_lane_lines.find_lane_pixels(lane_b_e_view, histo)\n",
    "\n",
    "    # Fit a polynomial to each line in pixels (left_fitx, right_fitx)\n",
    "    left_fitx, right_fitx = find_lane_lines.fit_polynomial(lane_b_e_view)\n",
    "\n",
    "    # Third use Search from Prior Polynomial for future frames:\n",
    "    # to avoid re-compute teadious sliding window method\n",
    "#     left_fitx, right_fitx = find_lane_lines.search_around_poly(lane_b_e_view)   \n",
    "\n",
    "    ##\n",
    "    # 6. Measure the Radius of Lane Curvature\n",
    "    ##\n",
    "    \n",
    "    calc_lane_curve = LaneLineCurvature()\n",
    "\n",
    "    # Get data from fit polynomial\n",
    "    ploty, left_fit, right_fit = find_lane_lines.get_fit_polynomial_data()\n",
    "\n",
    "    # Measures Lane Line Curvature\n",
    "    left_curverad, right_curverad, curverad_units = calc_lane_curve.measure_radius_curvature(ploty, left_fit, right_fit, \"meters\")\n",
    "\n",
    "    ##\n",
    "    # 7. Determine Vehicle's Position with Respect to Lane Center\n",
    "    ##\n",
    "    \n",
    "    lane_vehicle = LaneVehiclePosition()\n",
    "\n",
    "    # Measure Vehicle's position with respect to lane center\n",
    "    dist_center, position_units, side_center = lane_vehicle.measure_vehicle_position(lane_b_e_view, left_fit, right_fit, \"meters\")\n",
    "\n",
    "    ##\n",
    "    # 8. Overlay Lane Boundaries onto Undistorted Frame (Like Original)\n",
    "    ##\n",
    "    \n",
    "    lane_boundary = LaneBoundaries()\n",
    "\n",
    "    # Set binary frame lane_b_e_view in lane_boundary\n",
    "    lane_boundary.set_warped_binary_img(lane_b_e_view)\n",
    "\n",
    "    # Set undistorted frame undist_frame\n",
    "    lane_boundary.set_original_undist_img(undist_frame)\n",
    "\n",
    "    # Set attributes related to fit the lines with a polynomial\n",
    "    lane_boundary.set_fit_lines_poly(ploty, left_fit, right_fit)\n",
    "\n",
    "    # Get inverse perspective matrix\n",
    "    Minv = cam_view.get_minv()\n",
    "\n",
    "    # Set inverse perspective matrix (Minv)\n",
    "    lane_boundary.set_minv(Minv)\n",
    "\n",
    "    # Overlay Detected Lane Boundaries back onto undistorted frame\n",
    "    lane_boundary.overlay_lane_boundaries()\n",
    "    \n",
    "    ##\n",
    "    # 9. Add Lane Curvature and Vehicle Position onto Undist Frame\n",
    "    ##\n",
    "    \n",
    "    # Sets text properties to configure how text is displayed in image\n",
    "    font_family=cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_color=(255,255,255)\n",
    "    font_size=1.6\n",
    "    font_thickness = 3\n",
    "    line_type = cv2.LINE_AA\n",
    "    lane_boundary.set_img_text_properties(font_family, font_color, font_size, font_thickness, line_type)    \n",
    "    \n",
    "    # Sets left and right lane curvature radius and units\n",
    "    lane_boundary.set_lane_curvature_radius(left_curverad, right_curverad, curverad_units)\n",
    "\n",
    "    # Adds Lane Curvature Radius text to lane boundaries image\n",
    "    lane_boundary.overlay_lane_curvature()\n",
    "    \n",
    "    # Set Vehicle Position with respect to center of lane\n",
    "    lane_boundary.set_vehicle_position(dist_center, position_units, side_center)\n",
    "\n",
    "    # Adds Vehicle Position text to lane boundaries image\n",
    "    lane_boundary.overlay_vehicle_position()\n",
    "    \n",
    "    # Result of overlayed frame with lane boundaries, lane curvature\n",
    "    # and vehicle position\n",
    "    result = lane_boundary.get_overlayed_image()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Advanced Lane Finding Pipeline to Video Stream\n",
    "\n",
    "Uses **[scikit-video](www.scikit-video.org/stable/index.html)** to enrich each frame in the video for lane perception.\n",
    "\n",
    "How long will it take to enrich the video?\n",
    "3:00PM - 4:33PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Input Video Metadata for Writing Output Modified Video\n",
    "\n",
    "Use **scikit-video** to handle gathering input video metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_input_path = \"project_video.mp4\"\n",
    "\n",
    "# See Input Video Metadata, Its needed for writing Output Video\n",
    "metadata = skvideo.io.ffprobe(video_input_path)\n",
    "print(metadata.keys())\n",
    "print(json.dumps(metadata[\"video\"], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Output Video with Frames Modified\n",
    "\n",
    "Use **OpenCV** to handle reading input video frame by frame. Then use **OpenCV** to handle writing each modified frame to output video.\n",
    "\n",
    "Video Processing takes about **1 hour (15 - 25) minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output filename with its format\n",
    "video_mod_out_path = \"data/output/video/project_video_mod.mp4\"\n",
    "\n",
    "# Specify video metadata for JSON Path Extraction\n",
    "video_metadata = metadata[\"video\"]\n",
    "\n",
    "# Grab FourCC (video codec) based on input video: h264\n",
    "fourcc_name = video_metadata[\"@codec_name\"]\n",
    "\n",
    "# Grab the number of frames per second (FPS) based on input video\n",
    "frames_per_sec = int(Fraction(video_metadata[\"@avg_frame_rate\"]))\n",
    "\n",
    "# Specify frame size based on input video\n",
    "frame_width = int(video_metadata[\"@width\"])\n",
    "frame_height = int(video_metadata[\"@height\"])\n",
    "\n",
    "# Create a VideoWriter object (OpenCV)\n",
    "video_writer = cv2.VideoWriter(video_mod_out_path, cv2.VideoWriter_fourcc(*fourcc_name), frames_per_sec, (frame_width, frame_height))\n",
    "\n",
    "# Create a VideoReader object (Scikit-Video)\n",
    "# video_reader = skvideo.io.vreader(video_input_path)\n",
    "\n",
    "video_reader = cv2.VideoCapture(video_input_path)\n",
    "\n",
    "# Get total number of frames based on input video\n",
    "total_frames = int(video_metadata[\"@nb_frames\"])\n",
    "    \n",
    "# Loop while there are frames left to read from video_reader    \n",
    "for i in tqdm(range(total_frames)):\n",
    "    ret, frame = video_reader.read()\n",
    "    if ret == True:\n",
    "        # Modifying frame for lane perception by drawing lane boundary,\n",
    "        # radius of lane curvature and vehicle position with respect to\n",
    "        # center of lane\n",
    "        mod_frame = run_pipeline(frame)        \n",
    "        \n",
    "        # Write the frame into the file 'project_video_mod.mp4'\n",
    "        video_writer.write(mod_frame)\n",
    "    # Break the loop, video_reader.read() didn't return frame\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release video write object\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Video Mod Clip Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"{1}\" height=\"{2}\" controls>\n",
    "    <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(video_mod_out_path, frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
